{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "A sumarização de textos é muito útil para enfatizar uma ideia principal de um trecho de conteúdo. Dada uma entrada, isto é, o texto original, o algoritmo deve ser capaz de retornar uma versão reduzida contendo apenas as principais frases do texto, porém mantendo a linha de raciocínio original. Outra técnica frequentemente empregada na área de Processamento de Linguagem Natural é o cálculo de similaridade de textos, que nos permite comparar, dados dois textos e uma métrica, o quão similar eles são. Neste projeto, utilizamos ambas as técnicas para criar uma recomendação sumarizada para um vídeo de um conjunto pré definido.\n",
    "\n",
    "# Objetivo\n",
    "\n",
    "O presente trabalho possui dois objetivos principais:\n",
    "1. Gerar uma base de dados contendo informações sobre vídeos no Youtube, incluindo suas legendas no idioma inglês.\n",
    "2. Partindo-se de uma base gerada, criar uma recomendação sumarizada de outro vídeo semelhante ao vídeo de entrada. O vídeo de entrada pode não estar presente na base, porém a recomendação sempre estará na base.\n",
    "\n",
    "# Metodologia\n",
    "\n",
    "Para a geração da base, as legendas analisadas foram manualmente enviadas pelos criadores dos vídeos, e se limitam exclusivamente a versões no idioma inglês, pois é a linguagem mais utilizada na web, e consequentemente mais vídeos com legendas para este idioma estão disponíveis. Para isso, o dataset foi criado utilizando a técnica abordada no artigo *[\"Creating an NLP data set from YouTube subtitles\"](https://medium.com/@morga046/creating-an-nlp-data-set-from-youtube-subtitles-fb59c0955c2)*. Criamos uma token de autenticação na Google Cloud Platform para a API Youtube v3, responsável por retornar metadados sobre os vídeos públicos do site. Afim de evitar limites de requisição na API, delegamos o download das legendas para o programa [`youtube-dl`](https://youtube-dl.org/). Diferentemente do artigo, nosso dataset foi obtido a partir de diversas pesquisas aleatórias (opções como `criteria=random` na função `youtube_search`), para que várias categorias fossem incluídas. A biblioteca `webvtt` extraiu o texto das legendas, inicialmente no formato \"Web Video Text Tracks\", e por fim salvamos o conteúdo em uma planilha do Excel, `./bases_coletadas/ids.xslx`. \n",
    "A função `youtube_search` foi aplicada diversas vezes, para que no final obtivessemos uma base grande o suficiente com 16.066 linhas, uma para cada vídeo (e legenda) analisado. A funcão `clean_csv` então \"limpa\" o arquivo, removendo ids dos títulos dos vídeos das legendas e separando-as em um outro arquivo. Das linhas iniciais, apenas restaram 7457 itens, pois nem todos os vídeos possuíam legendas em inglês. Salvamos o dataset final em `./bases_coletadas/subtitles.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a obtenção do dataset, iniciamos a implantação do projeto em Python. Nele, utilizamos as bibliotecas `pandas`, `regex`, `numpy` e `sklearn`. Dessa forma, iniciamos a etapa do pré-processamento. Durante essa etapa, importamos a base gerada e criamos cópias para que pudéssemos realizar o procedimento desejado tanto com um Stemmer quanto com um Lemmatizer, avaliando as vantagens e desvantagens de cada um. Além disso, também foi criada uma `base_limpa`, utilizada posteriormente na etapa de sumarização das legendas. Realizamos, então, a normalização do texto e a extração de stopwords do corpus em inglês, incluindo também algumas onomatopéias customizadas. Foram removidos padrões que começavam com `&`, como por exemplo `&nsbp;` (um \"non-breaking space\" que aparece frequentemente nos arquivos de legendas) e outros elementos como `/b` e `/i`. Outros passos durante o pré-processamento foram a remoção de maiúsculas, substituição de números indicativos por tokens, indicações de interlocutor/sons especiais, símbolos especiais, caracteres especiais e remoção de excesso de espaços além da tokenização e aplicação de Stemming e Lemmatization nas bases para avaliação de qual seria mais eficiente dentro do algoritmo.\n",
    "Também ajustamos os títulos dos vídeos, removendo caracteres desnecessários, e retiramos textos com uma só palavra/letra.\n",
    "O pré-processamento levou ~725 segundos (12 minutos e 5 segundos) para o Stemmer e ~312 segundos (5 minutos e 12 segundos) para o Lemmatizer. Em seguida, foi feito o carregamento de bases já pré-processadas para agilizar a execução do algoritmo, e remoção de linhas vazias após pré-processamento.\n",
    "\n",
    "Na última etapa, ocorreu a geração das matrizes tfidf com os termos para cada uma das bases pré-processadas. Após a realização de testes, observamos que seria mais vantajoso utilizar o Stemming durante o pré-processamento, utilizando, portanto, a base pré-processada através dessa técnica. O número de itens restantes após a filtragem para ambos foi de 7.306 legendas. Neste momento, definimos funções gerar a sumarização de texto, com a funções responsáveis por quebrar o texto em frases, calcular similaridade entre sentenças (utilizando a métrica de distância de cosseno), gerar uma matriz de similaridade e, por fim, \"rankear\" as sentenças e uní-las em um resumo. Por fim, definimos a função `find_most_similar_any_video`, que compara a legenda de um vídeo de entrada (cuja legenda será obtida utilizando o `youtube-dl` novamente) com as legendas de todos os vídeos da base e retorna um resumo do vídeo com maior similaridade de cosseno. O resumo é criado a partir da técnica de sumarização, abrangendo muitas funções utilizadas em aula. Por fim, a fim de evitar problemas quanto ao download de programas externos etc., também definimos a função `find_most_similar_video`, que utiliza a mesma métrica para calcular uma matriz de similaridade entre os textos da base e, portanto, faz a recomendação apenas para textos de dentro da base.\n",
    "\n",
    "# Resultados Obtidos\n",
    "\n",
    "Os resultados obtidos foram satisfatórios, porém apresentaram algumas limitações. Observamos que o algoritmo apresenta sugestões bem semelhantes para um bom número de vídeos. No entanto, vídeos relacionados a genética, por exemplo, apresentaram sugestões de vídeos de esportes e gincanas e apontaram alta similaridade. Algumas possibilidades para explicar esse fenômeno residem no fato das legendas representarem a fala das pessoas, cuja interpretação pode ser altamente dependente do contexto e de outros sinais não verbais. Com relação à sumarização de texto, o resultado também foi satisfatório. Alguns desafios que encontramos foi a ausência de pontuações em alguns vídeos, o que prejudicou a geração dos resumos. A fim de contornar o problema, utilizamos símbolos especiais, como `&nbsp;` (non-breaking space), como pontuação, e assim conseguimos resultados melhores.\n",
    "Utiliando um exemplo com uma aula sobre política, uma aula de Big Data foi retornada, o que indica que a similaridade foi encontrada, pois o modelo percebeu a relação entre aulas, apesar de não encontrá-la entre tópicos. Um motivo pode ser a ausência de um vídeo mais específico sobre política em nossa base. Além disso, outro motivo pode ser o contexto específico com relação ao modo da fala do locutor, como ja comentado.\n",
    "Utilizando outro exemplo com um vídeo de malhação a resposta também foi outro vídeo de continuação do mesmo autor, com alta similaridade (92%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'oh',\n",
       " 'uh',\n",
       " 'na',\n",
       " 'gon',\n",
       " 'um',\n",
       " 'yeah',\n",
       " 'ih',\n",
       " 'meh',\n",
       " 'ai',\n",
       " 'hum',\n",
       " 'hehe',\n",
       " 'haha',\n",
       " 'hihi',\n",
       " 'hoho',\n",
       " 'hohoho']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install os\n",
    "import os\n",
    "#!pip install pandas\n",
    "import pandas as pd\n",
    "#!pip install numpy\n",
    "import numpy as np\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "#!pip install re\n",
    "import re\n",
    "#!pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "stopwords = nltk.corpus.stopwords.words('english') + ['oh', 'uh', 'na', 'gon', 'um', 'yeah', 'ih', 'meh', 'ai', 'hum', 'hehe', 'haha', 'hihi', 'hoho', 'hohoho'] #adicionando algumas expressões comuns que podem trazer problemas\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '\\\\'\n",
    "path_bases = path + 'bases_coletadas\\\\'\n",
    "\n",
    "base_legendas = pd.read_csv(path_bases + 'subtitles.csv').dropna(axis=0).drop('Unnamed: 0', axis=1).drop_duplicates(subset=['video_captions'], keep='first').reset_index(drop=True)\n",
    "base_legendas2 = base_legendas.copy(deep=True)\n",
    "base_limpa = base_legendas.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição de funções para pré-processamento\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stemming(lista):\n",
    "    retorno = []\n",
    "    for word in lista:\n",
    "        retorno.append(stemmer.stem(word))\n",
    "    return retorno\n",
    "\n",
    "def lemming(lista):\n",
    "    retorno = []\n",
    "    for word in lista:\n",
    "        retorno.append(lemmatizer.lemmatize(word))\n",
    "    return retorno\n",
    "\n",
    "def remover_simbolos(texto):\n",
    "    texto = re.sub(r'&.*?;\\w??&.*?;', ' ', texto) #retira caracteres presos entre &...; \n",
    "    texto = re.sub(r'&.*?;', ' ', texto) #retira &rslp; e afins\n",
    "    texto = re.sub(r'\\/\\w', ' ', texto) #retira /b, /i etc.\n",
    "    return texto\n",
    "    \n",
    "def preprocessamento(texto, reduc='stemming'):\n",
    "    #Removendo maiúsculas...\n",
    "    texto = texto.lower()\n",
    "    #Substituindo numeros por token indicativo...\n",
    "    texto = re.sub(r'[0-9]+', '_NUMBER_', texto)\n",
    "    #Removendo indicações de interlocutor/sons especiais...\n",
    "    texto = re.sub(r'[\\[\\(].*?[\\]\\)]', ' ', texto)\n",
    "    #Removendo símbolos especiais:\n",
    "    texto = remover_simbolos(texto)\n",
    "    #Removendo caracteres especiais...\n",
    "    texto = re.sub(r'[^a-z\\s]', ' ', texto)\n",
    "    texto = ' '.join(texto.split()) #retira excesso de espaços\n",
    "    #Tokenização...\n",
    "    tokens = nltk.tokenize.word_tokenize(texto)\n",
    "    #Removendo letras soltas...\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    #Removendo stopwords...\n",
    "    tokens_sw = [token for token in tokens if token not in stopwords]\n",
    "        #Aplicando Stemming\n",
    "    if reduc=='stemming':\n",
    "        tokens_sw = stemming(tokens_sw)\n",
    "    else:\n",
    "        #Aplicando Lemming\n",
    "        tokens_sw = lemming(tokens_sw)\n",
    "    return ' '.join(tokens_sw)\n",
    "\n",
    "def preprocessarbase(base, coluna, reduc='stemming'): #Executa o pré-processamento a todos os textos de uma base de dados\n",
    "    \n",
    "    base[coluna] = base[coluna].apply(lambda x: preprocessamento(x, reduc))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def ajustar_titulo(base, coluna): #Retira o ID e caracteres desnecessários dos títulos dos vídeos\n",
    "    base_clone = base.copy(deep=True)\n",
    "    titulos = list(base[coluna])\n",
    "    for i in range(len(titulos)):\n",
    "        titulo = titulos[i]\n",
    "        titulo = titulo.split('-')\n",
    "        titulo.pop(-1)\n",
    "        titulo = ' '.join(titulo)\n",
    "        titulo = re.sub(r'[^a-zA-Z#0-9_\\s]', '', titulo)\n",
    "        titulos[i] = titulo\n",
    "    base[coluna] = titulos\n",
    "    \n",
    "    return None\n",
    "\n",
    "def retirar_unicos(base, coluna): #retira textos com uma só palavra/letra\n",
    "        index = list(base.index)\n",
    "        remover = []\n",
    "        for i in index:\n",
    "            texto = base.loc[i, coluna]\n",
    "            if len(texto) <= 1 or len(texto.split()) <=1:\n",
    "                remover += [i]\n",
    "        base.drop(remover, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_captions</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#007_NewCityProductionsPodcast-CurtisMcCoshhow...</td>\n",
       "      <td>[Becky] Hi everybody welcome back to another e...</td>\n",
       "      <td>RX--L0xnnDo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#1-JoshGroban—TheLinkBetweenMusicandMentalHeal...</td>\n",
       "      <td>(upbeat music) - [Marjorie] Hi, this\\nis Marjo...</td>\n",
       "      <td>kNxmPdHVtp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#16ទៅប្រមូលក្រីស្តាល😁_ARK-SurvivalEvolved-QLd1...</td>\n",
       "      <td>Hello Guys\\nHope you guys enjoy this video\\nDo...</td>\n",
       "      <td>QLd1nGvjks4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#193-KetoandHeartHealthwithDr.BretScher-ZIoXUp...</td>\n",
       "      <td>Hey everyone, this is Geoffrey\\nWoo with the h...</td>\n",
       "      <td>ZIoXUpceBb4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#19HowtoorderfoodinSpanish(Dialogue)-_lv-ooUR4...</td>\n",
       "      <td>Yourspanishguide.com Episode number 29. Hello ...</td>\n",
       "      <td>_lv-ooUR4lw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_title  \\\n",
       "0  #007_NewCityProductionsPodcast-CurtisMcCoshhow...   \n",
       "1  #1-JoshGroban—TheLinkBetweenMusicandMentalHeal...   \n",
       "2  #16ទៅប្រមូលក្រីស្តាល😁_ARK-SurvivalEvolved-QLd1...   \n",
       "3  #193-KetoandHeartHealthwithDr.BretScher-ZIoXUp...   \n",
       "4  #19HowtoorderfoodinSpanish(Dialogue)-_lv-ooUR4...   \n",
       "\n",
       "                                      video_captions     video_id  \n",
       "0  [Becky] Hi everybody welcome back to another e...  RX--L0xnnDo  \n",
       "1  (upbeat music) - [Marjorie] Hi, this\\nis Marjo...  kNxmPdHVtp4  \n",
       "2  Hello Guys\\nHope you guys enjoy this video\\nDo...  QLd1nGvjks4  \n",
       "3  Hey everyone, this is Geoffrey\\nWoo with the h...  ZIoXUpceBb4  \n",
       "4  Yourspanishguide.com Episode number 29. Hello ...  _lv-ooUR4lw  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando a base\n",
    "base_legendas.head()\n",
    "base_legendas2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio do pré-processamento dos dados...\n",
      "Pré-processamento finalizado. Tempo decorrido: 724.9425637722015 segundos.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_captions</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#007_NewCityProductionsPodcast CurtisMcCoshhow...</td>\n",
       "      <td>hi everybodi welcom back anoth episod new citi...</td>\n",
       "      <td>RX--L0xnnDo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#1 JoshGrobanTheLinkBetweenMusicandMentalHealth</td>\n",
       "      <td>hi marjori morrison patrick kennedi welcom psy...</td>\n",
       "      <td>kNxmPdHVtp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#16_ARK SurvivalEvolved</td>\n",
       "      <td>hello guy hope guy enjoy video forget leav like</td>\n",
       "      <td>QLd1nGvjks4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#193 KetoandHeartHealthwithDrBretScher</td>\n",
       "      <td>hey everyon geoffrey woo health via modern nut...</td>\n",
       "      <td>ZIoXUpceBb4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#19HowtoorderfoodinSpanishDialogue _lv</td>\n",
       "      <td>yourspanishguid com episod number hello hello ...</td>\n",
       "      <td>_lv-ooUR4lw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_title  \\\n",
       "0  #007_NewCityProductionsPodcast CurtisMcCoshhow...   \n",
       "1    #1 JoshGrobanTheLinkBetweenMusicandMentalHealth   \n",
       "2                            #16_ARK SurvivalEvolved   \n",
       "3             #193 KetoandHeartHealthwithDrBretScher   \n",
       "4             #19HowtoorderfoodinSpanishDialogue _lv   \n",
       "\n",
       "                                      video_captions     video_id  \n",
       "0  hi everybodi welcom back anoth episod new citi...  RX--L0xnnDo  \n",
       "1  hi marjori morrison patrick kennedi welcom psy...  kNxmPdHVtp4  \n",
       "2    hello guy hope guy enjoy video forget leav like  QLd1nGvjks4  \n",
       "3  hey everyon geoffrey woo health via modern nut...  ZIoXUpceBb4  \n",
       "4  yourspanishguid com episod number hello hello ...  _lv-ooUR4lw  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aplicando pré-processamento na base e ajuste nos títulos com stemming\n",
    "from time import time\n",
    "print(\"Inicio do pré-processamento dos dados...\")\n",
    "inicio = time()\n",
    "preprocessarbase(base_legendas, 'video_captions')\n",
    "ajustar_titulo(base_legendas, 'video_title')\n",
    "retirar_unicos(base_legendas, 'video_captions')\n",
    "fim = time()\n",
    "print(\"Pré-processamento finalizado. Tempo decorrido: \" + str(fim-inicio) + \" segundos.\")\n",
    "base_legendas.to_csv(path_bases + 'Base_PREPROC_STEMMING.csv', index=False)\n",
    "\n",
    "base_legendas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio do pré-processamento dos dados...\n",
      "Pré-processamento finalizado. Tempo decorrido: 312.62347626686096 segundos.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_captions</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#007_NewCityProductionsPodcast CurtisMcCoshhow...</td>\n",
       "      <td>hi everybody welcome back another episode new ...</td>\n",
       "      <td>RX--L0xnnDo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#1 JoshGrobanTheLinkBetweenMusicandMentalHealth</td>\n",
       "      <td>hi marjorie morrison patrick kennedy welcome p...</td>\n",
       "      <td>kNxmPdHVtp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#16_ARK SurvivalEvolved</td>\n",
       "      <td>hello guy hope guy enjoy video forget leave like</td>\n",
       "      <td>QLd1nGvjks4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#193 KetoandHeartHealthwithDrBretScher</td>\n",
       "      <td>hey everyone geoffrey woo health via modern nu...</td>\n",
       "      <td>ZIoXUpceBb4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#19HowtoorderfoodinSpanishDialogue _lv</td>\n",
       "      <td>yourspanishguide com episode number hello hell...</td>\n",
       "      <td>_lv-ooUR4lw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_title  \\\n",
       "0  #007_NewCityProductionsPodcast CurtisMcCoshhow...   \n",
       "1    #1 JoshGrobanTheLinkBetweenMusicandMentalHealth   \n",
       "2                            #16_ARK SurvivalEvolved   \n",
       "3             #193 KetoandHeartHealthwithDrBretScher   \n",
       "4             #19HowtoorderfoodinSpanishDialogue _lv   \n",
       "\n",
       "                                      video_captions     video_id  \n",
       "0  hi everybody welcome back another episode new ...  RX--L0xnnDo  \n",
       "1  hi marjorie morrison patrick kennedy welcome p...  kNxmPdHVtp4  \n",
       "2   hello guy hope guy enjoy video forget leave like  QLd1nGvjks4  \n",
       "3  hey everyone geoffrey woo health via modern nu...  ZIoXUpceBb4  \n",
       "4  yourspanishguide com episode number hello hell...  _lv-ooUR4lw  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aplicando pré-processamento na base e ajuste nos títulos com lemming\n",
    "print(\"Inicio do pré-processamento dos dados...\")\n",
    "inicio = time()\n",
    "preprocessarbase(base_legendas2, 'video_captions', reduc='lemmatizing')\n",
    "ajustar_titulo(base_legendas2, 'video_title')\n",
    "retirar_unicos(base_legendas2, 'video_captions')\n",
    "fim = time()\n",
    "print(\"Pré-processamento finalizado. Tempo decorrido: \" + str(fim-inicio) + \" segundos.\")\n",
    "base_legendas2.to_csv(path_bases + 'Base_PREPROC_LEMMING.csv', index=False)\n",
    "base_legendas2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7293, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carregando bases já pré-processadas (para agilizar) e removendo vazios após pré-processamento\n",
    "#base_legendas = pd.read_csv(path_bases + 'Base_PREPROC_STEMMING.csv')\n",
    "#base_legendas2 = pd.read_csv(path_bases + 'Base_PREPROC_LEMMING.csv')\n",
    "\n",
    "base_legendas.dropna(inplace=True, axis=0)\n",
    "#base_legendas2.dropna(inplace=True, axis=0)\n",
    "\n",
    "base_legendas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7293, 161669)\n"
     ]
    }
   ],
   "source": [
    "#Gerando matrizes tf-idf com os termos para cada uma das bases pré-processadas\n",
    "#lemming_tfidf = TfidfVectorizer(stop_words='english')\n",
    "stemming_tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "XS_tfidf = stemming_tfidf.fit_transform(base_legendas['video_captions'])\n",
    "#XL_tfidf = lemming_tfidf.fit_transform(base_legendas2['video_captions'])\n",
    "\n",
    "print(XS_tfidf.shape)\n",
    "#A partir daqui, dados os testes que efetuamos, utilizaremos apenas a base com pré-processamento via stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo funções para sumarização de texto\n",
    "\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.regexp import regexp_tokenize\n",
    "#!pip install networkx\n",
    "import networkx as nx\n",
    "\n",
    "def read_text(text, n_sent=-1): #quebra texto em sentenças\n",
    "    retorno = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) == 1:\n",
    "        sentences = regexp_tokenize(text, r'&.*?;', gaps=True)\n",
    "    for sentence in sentences:\n",
    "        sentence = remover_simbolos(sentence)\n",
    "        retorno += [sentence.replace('[^a-zA-Z]', ' ').split()]\n",
    "    if len(retorno) < n_sent:\n",
    "        return retorno\n",
    "    \n",
    "    return retorno[:n_sent]\n",
    "    \n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None): #calcula similaridade entre duas sentenças\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    for w in sent1:\n",
    "        if w not in stopwords:\n",
    "            vector1[all_words.index(w)] += 1\n",
    " \n",
    "    for w in sent2:\n",
    "        if w not in stopwords:\n",
    "            vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)    \n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words): #cria matriz de similaridade com base nas sentenças\n",
    "\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 != idx2:\n",
    "                similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(file_name, top_n=5, n_sent=-1): #gera resumo com base nas sentenças mais similares\n",
    "    stop_words = stopwords\n",
    "    summarize_text = []    \n",
    "    \n",
    "    #Aplica tokenização do texto em sentenças\n",
    "    sentences =  read_text(file_name, n_sent=n_sent) \n",
    "    \n",
    "    #Constrói a matrix de similaridade\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)    \n",
    "\n",
    "    #Cria \"rank\" de sentenças\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)  \n",
    "    \n",
    "    #Pega as sentenças com maior rank\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    \n",
    "    if len(ranked_sentence) < top_n: #Para evitar erros em casos em que não é possível tokenizar o texto\n",
    "        top_n=1\n",
    "\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(' '.join(ranked_sentence[i][1]))\n",
    "\n",
    "    return ' '.join(summarize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Definindo função para recomendação de vídeos similares a partir de uma URL do YouTube.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#!pip install youtube-dl\n",
    "#!pip install webvtt-py\n",
    "import webvtt\n",
    "\n",
    "def get_captions(url): #Função que baixa a legenda de um vídeo de acordo com sua URL. Precisa da instalação do Youtube-DL\n",
    "    lang='en'\n",
    "    cmd = [\"youtube-dl\",\"--skip-download\",\"--write-sub\",\n",
    "               \"--sub-lang\",lang,url]\n",
    "    os.system(\" \".join(cmd))\n",
    "\n",
    "\n",
    "def convert_vtt(): #Converter a legenda de .vtt para .csv \n",
    "    try:\n",
    "        file = [filename for filename in os.listdir() if filename[-3:] == 'vtt'][0]\n",
    "    except:\n",
    "        print('This video has no subtitles =(')\n",
    "        return '_BREAK_'\n",
    "\n",
    "    #create an assets folder if one does not yet exist\n",
    "    if os.path.isdir('{}/assets'.format(os.getcwd())) == False:\n",
    "        os.makedirs('assets')\n",
    "    #extract the text and times from the vtt file\n",
    "    captions = webvtt.read(file)\n",
    "    text_time = pd.DataFrame()\n",
    "    text_time['text'] = [caption.text for caption in captions]\n",
    "    text_time['start'] = [caption.start for caption in captions]\n",
    "    text_time['stop'] = [caption.end for caption in captions]\n",
    "    text_time.to_csv('assets/{}.csv'.format(file[:-4]),index=False) #-4 to remove '.vtt'\n",
    "    #remove files from local drive\n",
    "    os.remove(file)\n",
    "    return file[:-4]\n",
    "\n",
    "def find_most_similar_any_video(url): #Compara a legenda do vídeo com a legenda de todos os vídeos da base e retorna um resumo do vídeo com maior similaridade.\n",
    "    similarity = []\n",
    "    get_captions(url)\n",
    "    file = convert_vtt()\n",
    "    if file == '_BREAK_':\n",
    "        return None\n",
    "        \n",
    "    video = pd.read_csv(path + 'assets\\\\' + file + '.csv')\n",
    "    video['text'] = video['text'].apply(lambda x: str(x))\n",
    "    text = stemming_tfidf.transform([preprocessamento(' '.join(video['text']))])\n",
    "    \n",
    "    for i in range(XS_tfidf.shape[0]):\n",
    "        row = XS_tfidf[i]\n",
    "        simi = cosine_similarity(text, row)[0][0]\n",
    "        if simi >= 0.99: #retira o próprio vídeo da contagem\n",
    "            similarity += [0.0]\n",
    "        else:\n",
    "            similarity += [simi]\n",
    "        \n",
    "    index = similarity.index(max(similarity))\n",
    "    \n",
    "    \n",
    "    id_video = base_limpa.loc[index, 'video_id']\n",
    "    url_video = 'https://www.youtube.com/watch?v=' + id_video\n",
    "    text_video = base_limpa.loc[index, 'video_captions']\n",
    "    print()\n",
    "    title_video = base_limpa.loc[index, 'video_title'].split('-')\n",
    "    title_video.pop()\n",
    "    title_video = ' '.join(title_video)\n",
    "    resumo=generate_summary(text_video, n_sent=500)\n",
    "    return title_video + '\\n\\n' + resumo  + '\\n\\n' + 'Available on: ' + url_video + '\\n\\n' + 'Similarity: ' + str(max(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CoronaVirus(COVID 19)discussionwithBillGates\n",
      "\n",
      "- Well, there's the league of, once you do the shutdown, you need to go, at least maybe two infection periods before you'd really expect to see things going down. And I'll add that question, maybe make it global, how do people balance, I've heard the argument that the economic harm could could cause a lot of deaths too and you could imagine in places like India, I've been monitoring some of the news there of folks who, with the shutdown, they have no livelihood, and they don't know how they're gonna get food, so that's happening even in the US, so how do you weigh those tensions, and what do you see as the dynamics that's keeping us from a really serious shutdown versus some of the more scattershot things that have been put in place? - Yeah, now we're gonna talk more about that. - Thank you so much, We could obviously talk for hours, I'm sure many people would love to hear there's, we could tell there's just so much more that we could learn especially about the virus. - Well, the next two months, hopefully we get that cases down, and have we're so good at that point, a quick turnaround, testing, contact tracing, testing, that we have been, and some ongoing measures like, no big public gathering, spacing, will must be part of that we don't know, we're trying to do the experiments to see how much that reduces transmission.\n",
      "\n",
      "Available on: https://www.youtube.com/watch?v=NDnjXdDxEhk\n",
      "\n",
      "Similarity: 0.7349805547805116\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar_any_video('https://www.youtube.com/watch?v=GOq8-FR8s1E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(XS_tfidf)\n",
    "def find_most_similar_video(url): #Para comparações apenas de vídeos dentro da base de forma mais rápida\n",
    "    values = []\n",
    "    video_id = re.findall(r'v=(.+)', url)[0]\n",
    "    video_index = list(base_legendas['video_id']).index(video_id)\n",
    "    \n",
    "    for index in range(XS_tfidf.shape[0]):\n",
    "        simi = similarity[video_index][index]\n",
    "        if simi >= 0.99:\n",
    "            values += [0.0]\n",
    "        else:\n",
    "            values += [simi]\n",
    "    index = values.index(max(values))\n",
    "    \n",
    "    id_video = base_limpa.loc[index, 'video_id']\n",
    "    url_video = 'https://www.youtube.com/watch?v=' + id_video\n",
    "    text_video = base_limpa.loc[index, 'video_captions']\n",
    "    title_video = base_limpa.loc[index, 'video_title'].split('-')\n",
    "    title_video.pop()\n",
    "    title_video = ' '.join(title_video)\n",
    "    resumo = generate_summary(text_video, n_sent=500)\n",
    "    return title_video + '\\n\\n' + resumo + '\\n\\n' + 'Available on: '+ url_video + '\\n\\n' + 'Similarity: ' + str(max(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullChairWorkout NoEquipment,Seated_MoreLifeHealth\n",
      "\n",
      "those shoulders up back and down and we're going to do that five times let's go one two three four and five - excellent work let's bring those arms up like this just to 90 degrees nice gentle posture what we're going to do are some shoulder circles backwards let's go one two and three - excellent work now we're just breathe keeping those arms pumping five seconds you can do it putting it in and three two and one - excellent work taking a deep breath in and out okay now we're going to go back to circles and let's go for one two three four and five excellent work bring the arms down like front of the arms five six seven eight nine - now let's go down slow now let's go one more coming up and down - excellent work let's shake out those arms taking a deep breath in and out now sitting with those shoulders back and down keeping that stomach tight let's face our hands\n",
      "\n",
      "Available on: https://www.youtube.com/watch?v=hzYCL86BFH8\n",
      "\n",
      "Similarity: 0.9269860720665256\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar_video('https://www.youtube.com/watch?v=PmMISnFqg8E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
