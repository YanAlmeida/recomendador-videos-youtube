{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdu√ß√£o\n",
    "\n",
    "A sumariza√ß√£o de textos √© muito √∫til para enfatizar uma ideia principal de um trecho de conte√∫do. Dada uma entrada, isto √©, o texto original, o algoritmo deve ser capaz de retornar uma vers√£o reduzida contendo apenas as principais frases do texto, por√©m mantendo a linha de racioc√≠nio original. Outra t√©cnica frequentemente empregada na √°rea de Processamento de Linguagem Natural √© o c√°lculo de similaridade de textos, que nos permite comparar, dados dois textos e uma m√©trica, o qu√£o similar eles s√£o. Neste projeto, utilizamos ambas as t√©cnicas para criar uma recomenda√ß√£o sumarizada para um v√≠deo de um conjunto pr√© definido.\n",
    "\n",
    "# Objetivo\n",
    "\n",
    "O presente trabalho possui dois objetivos principais:\n",
    "1. Gerar uma base de dados contendo informa√ß√µes sobre v√≠deos no Youtube, incluindo suas legendas no idioma ingl√™s.\n",
    "2. Partindo-se de uma base gerada, criar uma recomenda√ß√£o sumarizada de outro v√≠deo semelhante ao v√≠deo de entrada. O v√≠deo de entrada pode n√£o estar presente na base, por√©m a recomenda√ß√£o sempre estar√° na base.\n",
    "\n",
    "# Metodologia\n",
    "\n",
    "Para a gera√ß√£o da base, as legendas analisadas foram manualmente enviadas pelos criadores dos v√≠deos, e se limitam exclusivamente a vers√µes no idioma ingl√™s, pois √© a linguagem mais utilizada na web, e consequentemente mais v√≠deos com legendas para este idioma est√£o dispon√≠veis. Para isso, o dataset foi criado utilizando a t√©cnica abordada no artigo *[\"Creating an NLP data set from YouTube subtitles\"](https://medium.com/@morga046/creating-an-nlp-data-set-from-youtube-subtitles-fb59c0955c2)*. Criamos uma token de autentica√ß√£o na Google Cloud Platform para a API Youtube v3, respons√°vel por retornar metadados sobre os v√≠deos p√∫blicos do site. Afim de evitar limites de requisi√ß√£o na API, delegamos o download das legendas para o programa [`youtube-dl`](https://youtube-dl.org/). Diferentemente do artigo, nosso dataset foi obtido a partir de diversas pesquisas aleat√≥rias (op√ß√µes como `criteria=random` na fun√ß√£o `youtube_search`), para que v√°rias categorias fossem inclu√≠das. A biblioteca `webvtt` extraiu o texto das legendas, inicialmente no formato \"Web Video Text Tracks\", e por fim salvamos o conte√∫do em uma planilha do Excel, `./bases_coletadas/ids.xslx`. \n",
    "A fun√ß√£o `youtube_search` foi aplicada diversas vezes, para que no final obtivessemos uma base grande o suficiente com 16.066 linhas, uma para cada v√≠deo (e legenda) analisado. A func√£o `clean_csv` ent√£o \"limpa\" o arquivo, removendo ids dos t√≠tulos dos v√≠deos das legendas e separando-as em um outro arquivo. Das linhas iniciais, apenas restaram 7457 itens, pois nem todos os v√≠deos possu√≠am legendas em ingl√™s. Salvamos o dataset final em `./bases_coletadas/subtitles.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ap√≥s a obten√ß√£o do dataset, iniciamos a implanta√ß√£o do projeto em Python. Nele, utilizamos as bibliotecas `pandas`, `regex`, `numpy` e `sklearn`. Dessa forma, iniciamos a etapa do pr√©-processamento. Durante essa etapa, importamos a base gerada e criamos c√≥pias para que pud√©ssemos realizar o procedimento desejado tanto com um Stemmer quanto com um Lemmatizer, avaliando as vantagens e desvantagens de cada um. Al√©m disso, tamb√©m foi criada uma `base_limpa`, utilizada posteriormente na etapa de sumariza√ß√£o das legendas. Realizamos, ent√£o, a normaliza√ß√£o do texto e a extra√ß√£o de stopwords do corpus em ingl√™s, incluindo tamb√©m algumas onomatop√©ias customizadas. Foram removidos padr√µes que come√ßavam com `&`, como por exemplo `&nsbp;` (um \"non-breaking space\" que aparece frequentemente nos arquivos de legendas) e outros elementos como `/b` e `/i`. Outros passos durante o pr√©-processamento foram a remo√ß√£o de mai√∫sculas, substitui√ß√£o de n√∫meros indicativos por tokens, indica√ß√µes de interlocutor/sons especiais, s√≠mbolos especiais, caracteres especiais e remo√ß√£o de excesso de espa√ßos al√©m da tokeniza√ß√£o e aplica√ß√£o de Stemming e Lemmatization nas bases para avalia√ß√£o de qual seria mais eficiente dentro do algoritmo.\n",
    "Tamb√©m ajustamos os t√≠tulos dos v√≠deos, removendo caracteres desnecess√°rios, e retiramos textos com uma s√≥ palavra/letra.\n",
    "O pr√©-processamento levou ~725 segundos (12 minutos e 5 segundos) para o Stemmer e ~312 segundos (5 minutos e 12 segundos) para o Lemmatizer. Em seguida, foi feito o carregamento de bases j√° pr√©-processadas para agilizar a execu√ß√£o do algoritmo, e remo√ß√£o de linhas vazias ap√≥s pr√©-processamento.\n",
    "\n",
    "Na √∫ltima etapa, ocorreu a gera√ß√£o das matrizes tfidf com os termos para cada uma das bases pr√©-processadas. Ap√≥s a realiza√ß√£o de testes, observamos que seria mais vantajoso utilizar o Stemming durante o pr√©-processamento, utilizando, portanto, a base pr√©-processada atrav√©s dessa t√©cnica. O n√∫mero de itens restantes ap√≥s a filtragem para ambos foi de 7.306 legendas. Neste momento, definimos fun√ß√µes gerar a sumariza√ß√£o de texto, com a fun√ß√µes respons√°veis por quebrar o texto em frases, calcular similaridade entre senten√ßas (utilizando a m√©trica de dist√¢ncia de cosseno), gerar uma matriz de similaridade e, por fim, \"rankear\" as senten√ßas e un√≠-las em um resumo. Por fim, definimos a fun√ß√£o `find_most_similar_any_video`, que compara a legenda de um v√≠deo de entrada (cuja legenda ser√° obtida utilizando o `youtube-dl` novamente) com as legendas de todos os v√≠deos da base e retorna um resumo do v√≠deo com maior similaridade de cosseno. O resumo √© criado a partir da t√©cnica de sumariza√ß√£o, abrangendo muitas fun√ß√µes utilizadas em aula. Por fim, a fim de evitar problemas quanto ao download de programas externos etc., tamb√©m definimos a fun√ß√£o `find_most_similar_video`, que utiliza a mesma m√©trica para calcular uma matriz de similaridade entre os textos da base e, portanto, faz a recomenda√ß√£o apenas para textos de dentro da base.\n",
    "\n",
    "# Resultados Obtidos\n",
    "\n",
    "Os resultados obtidos foram satisfat√≥rios, por√©m apresentaram algumas limita√ß√µes. Observamos que o algoritmo apresenta sugest√µes bem semelhantes para um bom n√∫mero de v√≠deos. No entanto, v√≠deos relacionados a gen√©tica, por exemplo, apresentaram sugest√µes de v√≠deos de esportes e gincanas e apontaram alta similaridade. Algumas possibilidades para explicar esse fen√¥meno residem no fato das legendas representarem a fala das pessoas, cuja interpreta√ß√£o pode ser altamente dependente do contexto e de outros sinais n√£o verbais. Com rela√ß√£o √† sumariza√ß√£o de texto, o resultado tamb√©m foi satisfat√≥rio. Alguns desafios que encontramos foi a aus√™ncia de pontua√ß√µes em alguns v√≠deos, o que prejudicou a gera√ß√£o dos resumos. A fim de contornar o problema, utilizamos s√≠mbolos especiais, como `&nbsp;` (non-breaking space), como pontua√ß√£o, e assim conseguimos resultados melhores.\n",
    "Utiliando um exemplo com uma aula sobre pol√≠tica, uma aula de Big Data foi retornada, o que indica que a similaridade foi encontrada, pois o modelo percebeu a rela√ß√£o entre aulas, apesar de n√£o encontr√°-la entre t√≥picos. Um motivo pode ser a aus√™ncia de um v√≠deo mais espec√≠fico sobre pol√≠tica em nossa base. Al√©m disso, outro motivo pode ser o contexto espec√≠fico com rela√ß√£o ao modo da fala do locutor, como ja comentado.\n",
    "Utilizando outro exemplo com um v√≠deo de malha√ß√£o a resposta tamb√©m foi outro v√≠deo de continua√ß√£o do mesmo autor, com alta similaridade (92%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'oh',\n",
       " 'uh',\n",
       " 'na',\n",
       " 'gon',\n",
       " 'um',\n",
       " 'yeah',\n",
       " 'ih',\n",
       " 'meh',\n",
       " 'ai',\n",
       " 'hum',\n",
       " 'hehe',\n",
       " 'haha',\n",
       " 'hihi',\n",
       " 'hoho',\n",
       " 'hohoho']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install os\n",
    "import os\n",
    "#!pip install pandas\n",
    "import pandas as pd\n",
    "#!pip install numpy\n",
    "import numpy as np\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "#!pip install re\n",
    "import re\n",
    "#!pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "stopwords = nltk.corpus.stopwords.words('english') + ['oh', 'uh', 'na', 'gon', 'um', 'yeah', 'ih', 'meh', 'ai', 'hum', 'hehe', 'haha', 'hihi', 'hoho', 'hohoho'] #adicionando algumas express√µes comuns que podem trazer problemas\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '\\\\'\n",
    "path_bases = path + 'bases_coletadas\\\\'\n",
    "\n",
    "base_legendas = pd.read_csv(path_bases + 'subtitles.csv').dropna(axis=0).drop('Unnamed: 0', axis=1).drop_duplicates(subset=['video_captions'], keep='first').reset_index(drop=True)\n",
    "base_legendas2 = base_legendas.copy(deep=True)\n",
    "base_limpa = base_legendas.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defini√ß√£o de fun√ß√µes para pr√©-processamento\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stemming(lista):\n",
    "    retorno = []\n",
    "    for word in lista:\n",
    "        retorno.append(stemmer.stem(word))\n",
    "    return retorno\n",
    "\n",
    "def lemming(lista):\n",
    "    retorno = []\n",
    "    for word in lista:\n",
    "        retorno.append(lemmatizer.lemmatize(word))\n",
    "    return retorno\n",
    "\n",
    "def remover_simbolos(texto):\n",
    "    texto = re.sub(r'&.*?;\\w??&.*?;', ' ', texto) #retira caracteres presos entre &...; \n",
    "    texto = re.sub(r'&.*?;', ' ', texto) #retira &rslp; e afins\n",
    "    texto = re.sub(r'\\/\\w', ' ', texto) #retira /b, /i etc.\n",
    "    return texto\n",
    "    \n",
    "def preprocessamento(texto, reduc='stemming'):\n",
    "    #Removendo mai√∫sculas...\n",
    "    texto = texto.lower()\n",
    "    #Substituindo numeros por token indicativo...\n",
    "    texto = re.sub(r'[0-9]+', '_NUMBER_', texto)\n",
    "    #Removendo indica√ß√µes de interlocutor/sons especiais...\n",
    "    texto = re.sub(r'[\\[\\(].*?[\\]\\)]', ' ', texto)\n",
    "    #Removendo s√≠mbolos especiais:\n",
    "    texto = remover_simbolos(texto)\n",
    "    #Removendo caracteres especiais...\n",
    "    texto = re.sub(r'[^a-z\\s]', ' ', texto)\n",
    "    texto = ' '.join(texto.split()) #retira excesso de espa√ßos\n",
    "    #Tokeniza√ß√£o...\n",
    "    tokens = nltk.tokenize.word_tokenize(texto)\n",
    "    #Removendo letras soltas...\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    #Removendo stopwords...\n",
    "    tokens_sw = [token for token in tokens if token not in stopwords]\n",
    "        #Aplicando Stemming\n",
    "    if reduc=='stemming':\n",
    "        tokens_sw = stemming(tokens_sw)\n",
    "    else:\n",
    "        #Aplicando Lemming\n",
    "        tokens_sw = lemming(tokens_sw)\n",
    "    return ' '.join(tokens_sw)\n",
    "\n",
    "def preprocessarbase(base, coluna, reduc='stemming'): #Executa o pr√©-processamento a todos os textos de uma base de dados\n",
    "    \n",
    "    base[coluna] = base[coluna].apply(lambda x: preprocessamento(x, reduc))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def ajustar_titulo(base, coluna): #Retira o ID e caracteres desnecess√°rios dos t√≠tulos dos v√≠deos\n",
    "    base_clone = base.copy(deep=True)\n",
    "    titulos = list(base[coluna])\n",
    "    for i in range(len(titulos)):\n",
    "        titulo = titulos[i]\n",
    "        titulo = titulo.split('-')\n",
    "        titulo.pop(-1)\n",
    "        titulo = ' '.join(titulo)\n",
    "        titulo = re.sub(r'[^a-zA-Z#0-9_\\s]', '', titulo)\n",
    "        titulos[i] = titulo\n",
    "    base[coluna] = titulos\n",
    "    \n",
    "    return None\n",
    "\n",
    "def retirar_unicos(base, coluna): #retira textos com uma s√≥ palavra/letra\n",
    "        index = list(base.index)\n",
    "        remover = []\n",
    "        for i in index:\n",
    "            texto = base.loc[i, coluna]\n",
    "            if len(texto) <= 1 or len(texto.split()) <=1:\n",
    "                remover += [i]\n",
    "        base.drop(remover, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_captions</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#007_NewCityProductionsPodcast-CurtisMcCoshhow...</td>\n",
       "      <td>[Becky] Hi everybody welcome back to another e...</td>\n",
       "      <td>RX--L0xnnDo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#1-JoshGroban‚ÄîTheLinkBetweenMusicandMentalHeal...</td>\n",
       "      <td>(upbeat music) - [Marjorie] Hi, this\\nis Marjo...</td>\n",
       "      <td>kNxmPdHVtp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#16·ûë·üÖ·ûî·üí·ûö·ûò·ûº·ûõ·ûÄ·üí·ûö·û∏·ûü·üí·ûè·û∂·ûõüòÅ_ARK-SurvivalEvolved-QLd1...</td>\n",
       "      <td>Hello Guys\\nHope you guys enjoy this video\\nDo...</td>\n",
       "      <td>QLd1nGvjks4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#193-KetoandHeartHealthwithDr.BretScher-ZIoXUp...</td>\n",
       "      <td>Hey everyone, this is Geoffrey\\nWoo with the h...</td>\n",
       "      <td>ZIoXUpceBb4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#19HowtoorderfoodinSpanish(Dialogue)-_lv-ooUR4...</td>\n",
       "      <td>Yourspanishguide.com Episode number 29. Hello ...</td>\n",
       "      <td>_lv-ooUR4lw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_title  \\\n",
       "0  #007_NewCityProductionsPodcast-CurtisMcCoshhow...   \n",
       "1  #1-JoshGroban‚ÄîTheLinkBetweenMusicandMentalHeal...   \n",
       "2  #16·ûë·üÖ·ûî·üí·ûö·ûò·ûº·ûõ·ûÄ·üí·ûö·û∏·ûü·üí·ûè·û∂·ûõüòÅ_ARK-SurvivalEvolved-QLd1...   \n",
       "3  #193-KetoandHeartHealthwithDr.BretScher-ZIoXUp...   \n",
       "4  #19HowtoorderfoodinSpanish(Dialogue)-_lv-ooUR4...   \n",
       "\n",
       "                                      video_captions     video_id  \n",
       "0  [Becky] Hi everybody welcome back to another e...  RX--L0xnnDo  \n",
       "1  (upbeat music) - [Marjorie] Hi, this\\nis Marjo...  kNxmPdHVtp4  \n",
       "2  Hello Guys\\nHope you guys enjoy this video\\nDo...  QLd1nGvjks4  \n",
       "3  Hey everyone, this is Geoffrey\\nWoo with the h...  ZIoXUpceBb4  \n",
       "4  Yourspanishguide.com Episode number 29. Hello ...  _lv-ooUR4lw  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando a base\n",
    "base_legendas.head()\n",
    "base_legendas2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio do pr√©-processamento dos dados...\n",
      "Pr√©-processamento finalizado. Tempo decorrido: 724.9425637722015 segundos.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_captions</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#007_NewCityProductionsPodcast CurtisMcCoshhow...</td>\n",
       "      <td>hi everybodi welcom back anoth episod new citi...</td>\n",
       "      <td>RX--L0xnnDo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#1 JoshGrobanTheLinkBetweenMusicandMentalHealth</td>\n",
       "      <td>hi marjori morrison patrick kennedi welcom psy...</td>\n",
       "      <td>kNxmPdHVtp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#16_ARK SurvivalEvolved</td>\n",
       "      <td>hello guy hope guy enjoy video forget leav like</td>\n",
       "      <td>QLd1nGvjks4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#193 KetoandHeartHealthwithDrBretScher</td>\n",
       "      <td>hey everyon geoffrey woo health via modern nut...</td>\n",
       "      <td>ZIoXUpceBb4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#19HowtoorderfoodinSpanishDialogue _lv</td>\n",
       "      <td>yourspanishguid com episod number hello hello ...</td>\n",
       "      <td>_lv-ooUR4lw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_title  \\\n",
       "0  #007_NewCityProductionsPodcast CurtisMcCoshhow...   \n",
       "1    #1 JoshGrobanTheLinkBetweenMusicandMentalHealth   \n",
       "2                            #16_ARK SurvivalEvolved   \n",
       "3             #193 KetoandHeartHealthwithDrBretScher   \n",
       "4             #19HowtoorderfoodinSpanishDialogue _lv   \n",
       "\n",
       "                                      video_captions     video_id  \n",
       "0  hi everybodi welcom back anoth episod new citi...  RX--L0xnnDo  \n",
       "1  hi marjori morrison patrick kennedi welcom psy...  kNxmPdHVtp4  \n",
       "2    hello guy hope guy enjoy video forget leav like  QLd1nGvjks4  \n",
       "3  hey everyon geoffrey woo health via modern nut...  ZIoXUpceBb4  \n",
       "4  yourspanishguid com episod number hello hello ...  _lv-ooUR4lw  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aplicando pr√©-processamento na base e ajuste nos t√≠tulos com stemming\n",
    "from time import time\n",
    "print(\"Inicio do pr√©-processamento dos dados...\")\n",
    "inicio = time()\n",
    "preprocessarbase(base_legendas, 'video_captions')\n",
    "ajustar_titulo(base_legendas, 'video_title')\n",
    "retirar_unicos(base_legendas, 'video_captions')\n",
    "fim = time()\n",
    "print(\"Pr√©-processamento finalizado. Tempo decorrido: \" + str(fim-inicio) + \" segundos.\")\n",
    "base_legendas.to_csv(path_bases + 'Base_PREPROC_STEMMING.csv', index=False)\n",
    "\n",
    "base_legendas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio do pr√©-processamento dos dados...\n",
      "Pr√©-processamento finalizado. Tempo decorrido: 312.62347626686096 segundos.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_captions</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#007_NewCityProductionsPodcast CurtisMcCoshhow...</td>\n",
       "      <td>hi everybody welcome back another episode new ...</td>\n",
       "      <td>RX--L0xnnDo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#1 JoshGrobanTheLinkBetweenMusicandMentalHealth</td>\n",
       "      <td>hi marjorie morrison patrick kennedy welcome p...</td>\n",
       "      <td>kNxmPdHVtp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#16_ARK SurvivalEvolved</td>\n",
       "      <td>hello guy hope guy enjoy video forget leave like</td>\n",
       "      <td>QLd1nGvjks4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#193 KetoandHeartHealthwithDrBretScher</td>\n",
       "      <td>hey everyone geoffrey woo health via modern nu...</td>\n",
       "      <td>ZIoXUpceBb4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#19HowtoorderfoodinSpanishDialogue _lv</td>\n",
       "      <td>yourspanishguide com episode number hello hell...</td>\n",
       "      <td>_lv-ooUR4lw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_title  \\\n",
       "0  #007_NewCityProductionsPodcast CurtisMcCoshhow...   \n",
       "1    #1 JoshGrobanTheLinkBetweenMusicandMentalHealth   \n",
       "2                            #16_ARK SurvivalEvolved   \n",
       "3             #193 KetoandHeartHealthwithDrBretScher   \n",
       "4             #19HowtoorderfoodinSpanishDialogue _lv   \n",
       "\n",
       "                                      video_captions     video_id  \n",
       "0  hi everybody welcome back another episode new ...  RX--L0xnnDo  \n",
       "1  hi marjorie morrison patrick kennedy welcome p...  kNxmPdHVtp4  \n",
       "2   hello guy hope guy enjoy video forget leave like  QLd1nGvjks4  \n",
       "3  hey everyone geoffrey woo health via modern nu...  ZIoXUpceBb4  \n",
       "4  yourspanishguide com episode number hello hell...  _lv-ooUR4lw  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aplicando pr√©-processamento na base e ajuste nos t√≠tulos com lemming\n",
    "print(\"Inicio do pr√©-processamento dos dados...\")\n",
    "inicio = time()\n",
    "preprocessarbase(base_legendas2, 'video_captions', reduc='lemmatizing')\n",
    "ajustar_titulo(base_legendas2, 'video_title')\n",
    "retirar_unicos(base_legendas2, 'video_captions')\n",
    "fim = time()\n",
    "print(\"Pr√©-processamento finalizado. Tempo decorrido: \" + str(fim-inicio) + \" segundos.\")\n",
    "base_legendas2.to_csv(path_bases + 'Base_PREPROC_LEMMING.csv', index=False)\n",
    "base_legendas2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7293, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carregando bases j√° pr√©-processadas (para agilizar) e removendo vazios ap√≥s pr√©-processamento\n",
    "#base_legendas = pd.read_csv(path_bases + 'Base_PREPROC_STEMMING.csv')\n",
    "#base_legendas2 = pd.read_csv(path_bases + 'Base_PREPROC_LEMMING.csv')\n",
    "\n",
    "base_legendas.dropna(inplace=True, axis=0)\n",
    "#base_legendas2.dropna(inplace=True, axis=0)\n",
    "\n",
    "base_legendas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7293, 161669)\n"
     ]
    }
   ],
   "source": [
    "#Gerando matrizes tf-idf com os termos para cada uma das bases pr√©-processadas\n",
    "#lemming_tfidf = TfidfVectorizer(stop_words='english')\n",
    "stemming_tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "XS_tfidf = stemming_tfidf.fit_transform(base_legendas['video_captions'])\n",
    "#XL_tfidf = lemming_tfidf.fit_transform(base_legendas2['video_captions'])\n",
    "\n",
    "print(XS_tfidf.shape)\n",
    "#A partir daqui, dados os testes que efetuamos, utilizaremos apenas a base com pr√©-processamento via stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo fun√ß√µes para sumariza√ß√£o de texto\n",
    "\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.regexp import regexp_tokenize\n",
    "#!pip install networkx\n",
    "import networkx as nx\n",
    "\n",
    "def read_text(text, n_sent=-1): #quebra texto em senten√ßas\n",
    "    retorno = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) == 1:\n",
    "        sentences = regexp_tokenize(text, r'&.*?;', gaps=True)\n",
    "    for sentence in sentences:\n",
    "        sentence = remover_simbolos(sentence)\n",
    "        retorno += [sentence.replace('[^a-zA-Z]', ' ').split()]\n",
    "    if len(retorno) < n_sent:\n",
    "        return retorno\n",
    "    \n",
    "    return retorno[:n_sent]\n",
    "    \n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None): #calcula similaridade entre duas senten√ßas\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    for w in sent1:\n",
    "        if w not in stopwords:\n",
    "            vector1[all_words.index(w)] += 1\n",
    " \n",
    "    for w in sent2:\n",
    "        if w not in stopwords:\n",
    "            vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)    \n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words): #cria matriz de similaridade com base nas senten√ßas\n",
    "\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 != idx2:\n",
    "                similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(file_name, top_n=5, n_sent=-1): #gera resumo com base nas senten√ßas mais similares\n",
    "    stop_words = stopwords\n",
    "    summarize_text = []    \n",
    "    \n",
    "    #Aplica tokeniza√ß√£o do texto em senten√ßas\n",
    "    sentences =  read_text(file_name, n_sent=n_sent) \n",
    "    \n",
    "    #Constr√≥i a matrix de similaridade\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)    \n",
    "\n",
    "    #Cria \"rank\" de senten√ßas\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)  \n",
    "    \n",
    "    #Pega as senten√ßas com maior rank\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    \n",
    "    if len(ranked_sentence) < top_n: #Para evitar erros em casos em que n√£o √© poss√≠vel tokenizar o texto\n",
    "        top_n=1\n",
    "\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(' '.join(ranked_sentence[i][1]))\n",
    "\n",
    "    return ' '.join(summarize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Definindo fun√ß√£o para recomenda√ß√£o de v√≠deos similares a partir de uma URL do YouTube.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#!pip install youtube-dl\n",
    "#!pip install webvtt-py\n",
    "import webvtt\n",
    "\n",
    "def get_captions(url): #Fun√ß√£o que baixa a legenda de um v√≠deo de acordo com sua URL. Precisa da instala√ß√£o do Youtube-DL\n",
    "    lang='en'\n",
    "    cmd = [\"youtube-dl\",\"--skip-download\",\"--write-sub\",\n",
    "               \"--sub-lang\",lang,url]\n",
    "    os.system(\" \".join(cmd))\n",
    "\n",
    "\n",
    "def convert_vtt(): #Converter a legenda de .vtt para .csv \n",
    "    try:\n",
    "        file = [filename for filename in os.listdir() if filename[-3:] == 'vtt'][0]\n",
    "    except:\n",
    "        print('This video has no subtitles =(')\n",
    "        return '_BREAK_'\n",
    "\n",
    "    #create an assets folder if one does not yet exist\n",
    "    if os.path.isdir('{}/assets'.format(os.getcwd())) == False:\n",
    "        os.makedirs('assets')\n",
    "    #extract the text and times from the vtt file\n",
    "    captions = webvtt.read(file)\n",
    "    text_time = pd.DataFrame()\n",
    "    text_time['text'] = [caption.text for caption in captions]\n",
    "    text_time['start'] = [caption.start for caption in captions]\n",
    "    text_time['stop'] = [caption.end for caption in captions]\n",
    "    text_time.to_csv('assets/{}.csv'.format(file[:-4]),index=False) #-4 to remove '.vtt'\n",
    "    #remove files from local drive\n",
    "    os.remove(file)\n",
    "    return file[:-4]\n",
    "\n",
    "def find_most_similar_any_video(url): #Compara a legenda do v√≠deo com a legenda de todos os v√≠deos da base e retorna um resumo do v√≠deo com maior similaridade.\n",
    "    similarity = []\n",
    "    get_captions(url)\n",
    "    file = convert_vtt()\n",
    "    if file == '_BREAK_':\n",
    "        return None\n",
    "        \n",
    "    video = pd.read_csv(path + 'assets\\\\' + file + '.csv')\n",
    "    video['text'] = video['text'].apply(lambda x: str(x))\n",
    "    text = stemming_tfidf.transform([preprocessamento(' '.join(video['text']))])\n",
    "    \n",
    "    for i in range(XS_tfidf.shape[0]):\n",
    "        row = XS_tfidf[i]\n",
    "        simi = cosine_similarity(text, row)[0][0]\n",
    "        if simi >= 0.99: #retira o pr√≥prio v√≠deo da contagem\n",
    "            similarity += [0.0]\n",
    "        else:\n",
    "            similarity += [simi]\n",
    "        \n",
    "    index = similarity.index(max(similarity))\n",
    "    \n",
    "    \n",
    "    id_video = base_limpa.loc[index, 'video_id']\n",
    "    url_video = 'https://www.youtube.com/watch?v=' + id_video\n",
    "    text_video = base_limpa.loc[index, 'video_captions']\n",
    "    print()\n",
    "    title_video = base_limpa.loc[index, 'video_title'].split('-')\n",
    "    title_video.pop()\n",
    "    title_video = ' '.join(title_video)\n",
    "    resumo=generate_summary(text_video, n_sent=500)\n",
    "    return title_video + '\\n\\n' + resumo  + '\\n\\n' + 'Available on: ' + url_video + '\\n\\n' + 'Similarity: ' + str(max(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CoronaVirus(COVID 19)discussionwithBillGates\n",
      "\n",
      "- Well, there's the league of, once you do the shutdown, you need to go, at least maybe two infection periods before you'd really expect to see things going down. And I'll add that question, maybe make it global, how do people balance, I've heard the argument that the economic harm could could cause a lot of deaths too and you could imagine in places like India, I've been monitoring some of the news there of folks who, with the shutdown, they have no livelihood, and they don't know how they're gonna get food, so that's happening even in the US, so how do you weigh those tensions, and what do you see as the dynamics that's keeping us from a really serious shutdown versus some of the more scattershot things that have been put in place? - Yeah, now we're gonna talk more about that. - Thank you so much, We could obviously talk for hours, I'm sure many people would love to hear there's, we could tell there's just so much more that we could learn especially about the virus. - Well, the next two months, hopefully we get that cases down, and have we're so good at that point, a quick turnaround, testing, contact tracing, testing, that we have been, and some ongoing measures like, no big public gathering, spacing, will must be part of that we don't know, we're trying to do the experiments to see how much that reduces transmission.\n",
      "\n",
      "Available on: https://www.youtube.com/watch?v=NDnjXdDxEhk\n",
      "\n",
      "Similarity: 0.7349805547805116\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar_any_video('https://www.youtube.com/watch?v=GOq8-FR8s1E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(XS_tfidf)\n",
    "def find_most_similar_video(url): #Para compara√ß√µes apenas de v√≠deos dentro da base de forma mais r√°pida\n",
    "    values = []\n",
    "    video_id = re.findall(r'v=(.+)', url)[0]\n",
    "    video_index = list(base_legendas['video_id']).index(video_id)\n",
    "    \n",
    "    for index in range(XS_tfidf.shape[0]):\n",
    "        simi = similarity[video_index][index]\n",
    "        if simi >= 0.99:\n",
    "            values += [0.0]\n",
    "        else:\n",
    "            values += [simi]\n",
    "    index = values.index(max(values))\n",
    "    \n",
    "    id_video = base_limpa.loc[index, 'video_id']\n",
    "    url_video = 'https://www.youtube.com/watch?v=' + id_video\n",
    "    text_video = base_limpa.loc[index, 'video_captions']\n",
    "    title_video = base_limpa.loc[index, 'video_title'].split('-')\n",
    "    title_video.pop()\n",
    "    title_video = ' '.join(title_video)\n",
    "    resumo = generate_summary(text_video, n_sent=500)\n",
    "    return title_video + '\\n\\n' + resumo + '\\n\\n' + 'Available on: '+ url_video + '\\n\\n' + 'Similarity: ' + str(max(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullChairWorkout NoEquipment,Seated_MoreLifeHealth\n",
      "\n",
      "those shoulders up back and down and we're going to do that five times let's go one two three four and five - excellent work let's bring those arms up like this just to 90 degrees nice gentle posture what we're going to do are some shoulder circles backwards let's go one two and three - excellent work now we're just breathe keeping those arms pumping five seconds you can do it putting it in and three two and one - excellent work taking a deep breath in and out okay now we're going to go back to circles and let's go for one two three four and five excellent work bring the arms down like front of the arms five six seven eight nine - now let's go down slow now let's go one more coming up and down - excellent work let's shake out those arms taking a deep breath in and out now sitting with those shoulders back and down keeping that stomach tight let's face our hands\n",
      "\n",
      "Available on: https://www.youtube.com/watch?v=hzYCL86BFH8\n",
      "\n",
      "Similarity: 0.9269860720665256\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar_video('https://www.youtube.com/watch?v=PmMISnFqg8E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
